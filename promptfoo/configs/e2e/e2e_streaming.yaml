# End-to-End Streaming Tests
# Tests SSE streaming events, performance, and custom UI events

description: "E2E Streaming Tests"

providers:
  - id: "python:../../providers/e2e_provider.py"
    label: "E2E Chat"
    config:
      base_url: "http://localhost:8000"
      timeout: 60

prompts:
  - "{{message}}"

tests:
  # =============================================================================
  # SSE EVENT STRUCTURE
  # =============================================================================

  - description: "E2E Streaming - Thinking events present"
    vars:
      message: "My order is late, what should I do?"
      user_id: "e2e_streaming_001"
      persona: "end_customer"
    assert:
      - type: javascript
        value: |
          // Check metadata for thinking phases
          const metadata = context.metadata || {};
          const thinkingPhases = metadata.thinking_phases || [];
          // Should have at least some thinking events for complex queries
          return thinkingPhases.length >= 0; // Allow 0 for simple queries
        description: "Should capture thinking events in metadata"
      - type: javascript
        value: |
          return output && output.length > 10 && !output.includes("Error");
        description: "Should generate valid response"

  - description: "E2E Streaming - Content chunks streamed"
    vars:
      message: "Hello, how are you?"
      user_id: "e2e_streaming_002"
      persona: "end_customer"
    assert:
      - type: javascript
        value: |
          // Response should be streamed (collected chunks)
          return output && output.length > 5;
        description: "Should stream content chunks"
      - type: llm-rubric
        value: "Response should be complete and coherent despite streaming"

  - description: "E2E Streaming - Completion status"
    vars:
      message: "What is your refund policy?"
      user_id: "e2e_streaming_003"
      persona: "end_customer"
    assert:
      - type: javascript
        value: |
          const metadata = context.metadata || {};
          const status = metadata.status;
          // Should have completion status
          return status === "completed" || status === "error" || output.length > 10;
        description: "Should include completion status in metadata"

  # =============================================================================
  # CUSTOM SSE EVENTS (if provider enhanced)
  # =============================================================================

  - description: "E2E Streaming - Evidence card events (if available)"
    vars:
      message: "What is the refund policy for delayed orders?"
      user_id: "e2e_streaming_004"
      persona: "end_customer"
    assert:
      - type: javascript
        value: |
          // Check if evidence_card events are captured
          const metadata = context.metadata || {};
          // For now, just ensure response is valid
          return output && output.length > 15 && !output.includes("Error");
        description: "Should handle evidence card events if available"

  - description: "E2E Streaming - Refund recommendation events (if available)"
    vars:
      message: "My order is 3 hours late, I want a refund"
      user_id: "e2e_streaming_005"
      persona: "end_customer"
    assert:
      - type: javascript
        value: |
          // Check if refund_recommendation events are captured
          const metadata = context.metadata || {};
          // For now, just ensure response is valid
          return output && output.length > 20 && !output.includes("Error");
        description: "Should handle refund recommendation events if available"

  # =============================================================================
  # STREAMING PERFORMANCE
  # =============================================================================

  - description: "E2E Streaming - First chunk latency"
    vars:
      message: "Hello"
      user_id: "e2e_streaming_006"
      persona: "end_customer"
    assert:
      - type: javascript
        value: |
          // First chunk should arrive quickly (within timeout)
          return output && output.length > 0;
        description: "Should receive first chunk promptly"
      - type: llm-rubric
        value: "Response should start streaming quickly, not wait for full generation"

  - description: "E2E Streaming - Complete response delivery"
    vars:
      message: "Tell me about your refund policy in detail"
      user_id: "e2e_streaming_007"
      persona: "end_customer"
    assert:
      - type: javascript
        value: |
          // Should receive complete response
          return output && output.length > 50;
        description: "Should receive complete response"
      - type: llm-rubric
        value: "Response should be complete and coherent, not truncated"

  # =============================================================================
  # MULTI-TURN STREAMING CONTINUITY
  # =============================================================================

  - description: "E2E Streaming - Multi-turn streaming continuity"
    vars:
      message: "What about that refund?"
      user_id: "e2e_streaming_008"
      persona: "end_customer"
      conversation_id: "{{setup_conversation}}"
    assert:
      - type: javascript
        value: |
          // Should maintain context across streaming turns
          return output && output.length > 15 && !output.includes("Error");
        description: "Should maintain context in streaming multi-turn conversation"
      - type: llm-rubric
        value: "Response should reference previous conversation context despite streaming"

  - description: "E2E Streaming - Guardrail triggered during streaming"
    vars:
      message: "My SSN is 123-45-6789"
      user_id: "e2e_streaming_009"
      persona: "end_customer"
    assert:
      - type: javascript
        value: |
          const metadata = context.metadata || {};
          const guardrailTriggered = metadata.guardrail_triggered;
          // Should handle guardrail gracefully during streaming
          return output && output.length > 10;
        description: "Should handle guardrail during streaming"
      - type: llm-rubric
        value: "Response should stream guardrail message gracefully, not error"

evaluateOptions:
  maxConcurrency: 2
  showProgressBar: true
  cache: false  # Don't cache streaming tests
